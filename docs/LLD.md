# flog - Fast Log Filter CLI Tool

## Low-Level Design Document

---

## 1. Overview

**flog** is a high-performance CLI tool for filtering structured logs by multiple fields with chainable filters. It's designed to handle large log files efficiently through streaming and parallel processing.

### Key Goals
- ðŸ” Multi-field filtering with AND/OR logic
- âš¡ Instant filtering of large files (streaming, no full file load)
- ðŸŽ¯ Schema-agnostic (works with any JSON structure)
- ðŸ› ï¸ Unix-friendly (pipes, stdin support)

---

## 2. Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CLI Layer                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Args      â”‚  â”‚   Help/      â”‚  â”‚   Config               â”‚  â”‚
â”‚  â”‚   Parser    â”‚  â”‚   Examples   â”‚  â”‚   (future)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Query Engine                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Query     â”‚  â”‚   Filter     â”‚  â”‚   Matcher              â”‚  â”‚
â”‚  â”‚   Parser    â”‚  â”‚   Chain      â”‚  â”‚   (exact/regex/range)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Core Engine                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Log       â”‚  â”‚   Streaming  â”‚  â”‚   Parallel             â”‚  â”‚
â”‚  â”‚   Parser    â”‚  â”‚   Reader     â”‚  â”‚   Processor            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Output Layer                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Pretty    â”‚  â”‚   JSON       â”‚  â”‚   Stats/               â”‚  â”‚
â”‚  â”‚   Printer   â”‚  â”‚   Output     â”‚  â”‚   Count                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Module Design

### 3.1 Log Parser (`internal/parser`)

**Responsibility:** Parse log lines into queryable structures

```go
// LogEntry represents a parsed log line
type LogEntry struct {
    Raw      string                 // Original line
    Fields   map[string]any         // Parsed fields (flattened)
    LineNum  int                    // Line number in file
}

// Parser interface for different log formats
type Parser interface {
    Parse(line string) (*LogEntry, error)
    CanParse(line string) bool
}

// Supported parsers:
// - JSONParser     â†’ {"level": "error", "user": {"id": 123}}
// - KeyValueParser â†’ level=error user.id=123
// - AutoParser     â†’ Auto-detect format per line
```

**Field Flattening:**
```
Input:  {"user": {"profile": {"name": "john"}}}
Output: map["user.profile.name"] = "john"
```

### 3.2 Filter Engine (`internal/filter`)

**Responsibility:** Match log entries against filter conditions

```go
// Condition represents a single filter condition
type Condition struct {
    Field    string      // e.g., "user.id", "level"
    Operator Operator    // EQ, NE, GT, LT, REGEX, CONTAINS
    Value    any         // Target value
}

// Operator types
type Operator int
const (
    OpEq       Operator = iota  // field:value or field=value
    OpNe                        // field!=value
    OpGt                        // field>value
    OpLt                        // field<value
    OpGte                       // field>=value
    OpLte                       // field<=value
    OpRegex                     // field~=pattern
    OpContains                  // field*=substring
    OpExists                    // field?
)

// FilterChain represents AND/OR combinations
type FilterChain struct {
    Conditions []Condition
    Logic      Logic  // AND / OR
}

// Matcher evaluates conditions against entries
type Matcher interface {
    Match(entry *LogEntry, chain *FilterChain) bool
}
```

### 3.3 Query DSL (`internal/filter/query.go`)

**Syntax Design:**

```bash
# Basic equality (AND by default)
flog -f "level:error,status:500" access.log

# Explicit operators
flog -f "status>=400,status<500" access.log

# OR conditions (use | separator)
flog -f "level:error|level:warn" access.log

# Mixed AND/OR (parentheses for grouping)
flog -f "(level:error|level:warn),status:500" access.log

# Nested fields
flog -f "user.profile.role:admin" access.log

# Regex matching
flog -f "message~=timeout.*retry" access.log

# Existence check
flog -f "error?" access.log  # Has 'error' field

# Negation
flog -f "level!=debug" access.log
```

**Query Grammar (BNF):**
```
query      â†’ group ("," group)*
group      â†’ condition ("|" condition)*
condition  â†’ field operator value
field      â†’ identifier ("." identifier)*
operator   â†’ ":" | "=" | "!=" | ">" | "<" | ">=" | "<=" | "~=" | "*=" | "?"
value      â†’ string | number | boolean
```

### 3.4 Streaming Reader (`internal/parser/reader.go`)

**Responsibility:** Read large files without memory bloat

```go
// StreamReader reads files line by line
type StreamReader struct {
    bufferSize int  // Default: 64KB per line buffer
}

func (r *StreamReader) Read(path string) <-chan string {
    // Returns channel that yields lines
    // Supports: regular files, gzip, stdin
}

// For parallel processing
func (r *StreamReader) ReadChunks(path string, chunkSize int) <-chan []string {
    // Returns channel of line batches for worker pools
}
```

### 3.5 Parallel Processor (`internal/filter/parallel.go`)

**Strategy:** Fan-out/fan-in with worker pools

```go
type ParallelFilter struct {
    Workers    int          // Default: runtime.NumCPU()
    ChunkSize  int          // Lines per chunk (default: 1000)
}

func (p *ParallelFilter) Filter(
    input <-chan []string,
    chain *FilterChain,
) <-chan *LogEntry {
    // 1. Spawn N workers
    // 2. Each worker parses + filters a chunk
    // 3. Results merged into output channel
}
```

**Pipeline Flow:**
```
File â†’ [Chunk Reader] â†’ [Worker Pool] â†’ [Merger] â†’ Output
           â”‚                  â”‚
           â”‚            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
           â”‚            â”‚ Worker 1  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Worker 2  â”‚â”€â”€â”€â”€â”€â”€â”€â–¶ Results
                        â”‚ Worker N  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.6 Output Formatter (`internal/output`)

```go
type Formatter interface {
    Format(entry *LogEntry) string
}

// Implementations:
// - RawFormatter    â†’ Original line
// - PrettyFormatter â†’ Colorized, indented JSON
// - JSONFormatter   â†’ Compact JSON
// - FieldsFormatter â†’ Only selected fields

type OutputMode int
const (
    ModeLines  OutputMode = iota  // Print matching lines
    ModeCount                     // Print count only
    ModeStats                     // Print field statistics
    ModeFirst                     // Print first N matches
)
```

---

## 4. CLI Interface

### 4.1 Command Structure

```bash
flog [OPTIONS] <FILE>...

Arguments:
  <FILE>...  Log file(s) to filter (use - for stdin)

Options:
  -f, --filter <QUERY>      Filter expression (required)
  -o, --output <FORMAT>     Output format: raw|pretty|json|fields [default: raw]
  -c, --count               Print match count only
  -n, --limit <N>           Limit output to first N matches
  -F, --fields <FIELDS>     Select specific fields to output
  -i, --ignore-case         Case-insensitive matching
  -v, --invert              Invert match (print non-matching)
  -j, --jobs <N>            Parallel workers [default: CPU count]
      --stats               Print field statistics
  -h, --help                Print help
  -V, --version             Print version

Examples:
  flog -f "level:error" app.log
  flog -f "status>=400,method:POST" access.log
  flog -f "user.id:123|user.id:456" --output pretty events.json
  cat app.log | flog -f "error?" -
  flog -f "level:error" --count *.log
```

### 4.2 Example Workflows

```bash
# Find all errors with specific user
flog -f "level:error,user.id:12345" app.log

# Count 5xx errors per file
flog -f "status>=500" --count access-*.log

# Pretty print warnings from last hour (with jq pre-filter)
cat app.log | jq -c 'select(.timestamp > "2024-01-01T12:00:00")' | flog -f "level:warn" -o pretty -

# Extract specific fields
flog -f "level:error" -F "timestamp,message,stack" app.log

# Chain with other tools
flog -f "status:500" access.log | flog -f "path~=/api/users" -
```

---

## 5. Data Structures

### 5.1 Core Types

```go
// Entry pool for memory efficiency
var entryPool = sync.Pool{
    New: func() interface{} {
        return &LogEntry{
            Fields: make(map[string]any, 16),
        }
    },
}

// Compiled query for reuse
type CompiledQuery struct {
    Chain       *FilterChain
    RegexCache  map[string]*regexp.Regexp
}
```

### 5.2 Result Statistics

```go
type Stats struct {
    TotalLines   int64
    MatchedLines int64
    ParseErrors  int64
    Duration     time.Duration
    FieldCounts  map[string]int64  // For --stats mode
}
```

---

## 6. Performance Considerations

### 6.1 Memory Management
- **Streaming:** Never load entire file; process line by line
- **Object Pooling:** Reuse `LogEntry` objects via `sync.Pool`
- **Buffer Reuse:** Reuse byte buffers for parsing

### 6.2 CPU Optimization
- **Parallel Processing:** Worker pool sized to CPU cores
- **Regex Caching:** Compile regex patterns once
- **Short-circuit Evaluation:** Stop evaluating AND conditions on first false
- **SIMD (future):** Use SIMD for string matching where applicable

### 6.3 I/O Optimization
- **Large Buffers:** 64KB+ read buffers
- **Async I/O:** Read ahead while processing
- **Memory-mapped Files (optional):** For random access patterns

### 6.4 Benchmarks Target
| File Size | Target Time |
|-----------|-------------|
| 100 MB    | < 1 second  |
| 1 GB      | < 5 seconds |
| 10 GB     | < 30 seconds|

---

## 7. File Structure

```
flog/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ flog/
â”‚       â””â”€â”€ main.go           # CLI entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ parser/
â”‚   â”‚   â”œâ”€â”€ parser.go         # Parser interface
â”‚   â”‚   â”œâ”€â”€ json.go           # JSON log parser
â”‚   â”‚   â”œâ”€â”€ keyvalue.go       # Key-value parser
â”‚   â”‚   â”œâ”€â”€ auto.go           # Auto-detection
â”‚   â”‚   â””â”€â”€ reader.go         # Streaming file reader
â”‚   â”œâ”€â”€ filter/
â”‚   â”‚   â”œâ”€â”€ condition.go      # Filter conditions
â”‚   â”‚   â”œâ”€â”€ chain.go          # Filter chain (AND/OR)
â”‚   â”‚   â”œâ”€â”€ matcher.go        # Matching logic
â”‚   â”‚   â”œâ”€â”€ query.go          # Query DSL parser
â”‚   â”‚   â””â”€â”€ parallel.go       # Parallel processing
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ formatter.go      # Output interface
â”‚       â”œâ”€â”€ raw.go            # Raw output
â”‚       â”œâ”€â”€ pretty.go         # Pretty printed
â”‚       â”œâ”€â”€ json.go           # JSON output
â”‚       â””â”€â”€ stats.go          # Statistics output
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â”œâ”€â”€ README.md
â””â”€â”€ LLD.md
```

---

## 8. Dependencies

```go
// go.mod
require (
    github.com/spf13/cobra v1.8.0    // CLI framework
    github.com/fatih/color v1.16.0   // Terminal colors
    github.com/json-iterator/go v1.1.12  // Fast JSON parsing
)
```

---

## 9. Future Enhancements

- [ ] Config file support (~/.flogrc)
- [ ] Saved filter aliases
- [ ] Time range filters (--since, --until)
- [ ] Aggregation mode (group by field)
- [ ] Watch mode (tail -f equivalent)
- [ ] Compressed file support (.gz, .zst)
- [ ] Multi-line log support
- [ ] Field type inference and casting

---

## 10. Testing Strategy

### Unit Tests
- Parser: Various JSON/KV formats
- Filter: All operator types
- Query DSL: Grammar edge cases

### Integration Tests
- End-to-end CLI tests
- Large file performance tests
- Pipe/stdin handling

### Benchmarks
```go
func BenchmarkFilter1GB(b *testing.B) { ... }
func BenchmarkParallelVsSerial(b *testing.B) { ... }
```

